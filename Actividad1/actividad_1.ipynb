{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Lectura y analisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opciones disponibles para pd.read_csv:\n",
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "\n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "\n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "\n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
      "        C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator from only the first valid\n",
      "        row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
      "        In addition, separators longer than 1 character and different from\n",
      "        ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
      "        the use of the Python parsing engine. Note that regex delimiters are prone\n",
      "        to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, optional\n",
      "        Alias for ``sep``.\n",
      "    header : int, Sequence of int, 'infer' or None, default 'infer'\n",
      "        Row number(s) containing column labels and marking the start of the\n",
      "        data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly to ``names`` then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
      "        e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : Sequence of Hashable, optional\n",
      "        Sequence of column labels to apply. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : Hashable, Sequence of Hashable or False, optional\n",
      "      Column(s) to use as row label(s), denoted either by column labels or column\n",
      "      indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
      "      will be formed for the row labels.\n",
      "\n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g., when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : Sequence of Hashable or Callable, optional\n",
      "        Subset of columns to select, denoted either by column labels or column indices.\n",
      "        If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in ``names`` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
      "        preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
      "        for columns in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to ``True``. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    dtype : dtype or dict of {Hashable : dtype}, optional\n",
      "        Data type(s) to apply to either the whole dataset or individual columns.\n",
      "        E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n",
      "        Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
      "        to preserve and not interpret ``dtype``.\n",
      "        If ``converters`` are specified, they will be applied INSTEAD\n",
      "        of ``dtype`` conversion.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "            Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
      "            the default determines the ``dtype`` of the columns which are not explicitly\n",
      "            listed.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict of {Hashable : Callable}, optional\n",
      "        Functions for converting values in specified columns. Keys can either\n",
      "        be column labels or column indices.\n",
      "    true_values : list, optional\n",
      "        Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
      "    false_values : list, optional\n",
      "        Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : int, list of int or Callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
      "        at the start of the file.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
      "        Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
      "        per-column ``NA`` values.  By default the following values are interpreted as\n",
      "        ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n",
      "        \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n",
      "        \"n/a\", \"nan\", \"null \".\n",
      "\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default ``NaN`` values when parsing the data.\n",
      "        Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
      "\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
      "          is appended to the default ``NaN`` values used for parsing.\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
      "          the default ``NaN`` values are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
      "          the ``NaN`` values specified ``na_values`` are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
      "          strings will be parsed as ``NaN``.\n",
      "\n",
      "        Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
      "        ``na_values`` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of ``na_values``). In\n",
      "        data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
      "        performance of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of ``NA`` values placed in non-numeric columns.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "    skip_blank_lines : bool, default True\n",
      "        If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
      "    parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
      "        The behavior is as follows:\n",
      "\n",
      "        * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
      "          ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
      "        * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
      "          as a single date column. Values are joined with a space before parsing.\n",
      "        * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n",
      "          result 'foo'. Values are joined with a space before parsing.\n",
      "\n",
      "        If a column or index cannot be represented as an array of ``datetime``,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an ``object`` data type. For\n",
      "        non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
      "        :func:`~pandas.read_csv`.\n",
      "\n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
      "        format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "            A strict version of this argument is now the default, passing it has no effect.\n",
      "\n",
      "    keep_date_col : bool, default False\n",
      "        If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : Callable, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. pandas will try to call ``date_parser`` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by ``parse_dates`` into a single array\n",
      "        and pass that; and 3) call ``date_parser`` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by ``parse_dates``) as\n",
      "        arguments.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "           Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "           :func:`~pandas.to_datetime` as-needed.\n",
      "    date_format : str or dict of column -> format, optional\n",
      "        Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
      "        The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "        `strftime documentation\n",
      "        <https://docs.python.org/3/library/datetime.html\n",
      "        #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "        note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "        You can also pass:\n",
      "\n",
      "        - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "            time string (not necessarily in exactly the same format);\n",
      "        - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "            and you should probably use it along with `dayfirst`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "\n",
      "    iterator : bool, default False\n",
      "        Return ``TextFileReader`` object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Number of lines to read from the file per chunk. Passing a value will cause the\n",
      "        function to return a ``TextFileReader`` object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
      "        Set to ``None`` for no decompression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "        other key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "\n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "    thousands : str (length 1), optional\n",
      "        Character acting as the thousands separator in numerical values.\n",
      "    decimal : str (length 1), default '.'\n",
      "        Character to recognize as decimal point (e.g., use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character used to denote a line break. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        Character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the ``delimiter`` and it will be ignored.\n",
      "    quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
      "        ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
      "        characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
      "        or ``lineterminator``.\n",
      "    doublequote : bool, default True\n",
      "       When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        Character used to escape other characters.\n",
      "    comment : str (length 1), optional\n",
      "        Character indicating that the remainder of line should not be parsed.\n",
      "        If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter ``header`` but not by\n",
      "        ``skiprows``. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
      "        treated as the header.\n",
      "    encoding : str, optional, default 'utf-8'\n",
      "        Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "\n",
      "    encoding_errors : str, optional, default 'strict'\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
      "        ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
      "        override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
      "        documentation for more details.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "\n",
      "        - ``'error'``, raise an Exception when a bad line is encountered.\n",
      "        - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
      "        - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new ``list`` of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine='python'``\n",
      "\n",
      "        .. versionchanged:: 2.2.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              as described in `pyarrow documentation\n",
      "              <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
      "              #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
      "\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
      "        used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to ``True``, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "            Use ``sep=\"\\s+\"`` instead.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
      "        Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
      "        regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
      "        chunks. (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : {'high', 'legacy', 'round_trip'}, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
      "        ``'legacy'`` for the original lower precision pandas converter, and\n",
      "        ``'round_trip'`` for the round-trip converter.\n",
      "\n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "    dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
      "        Back-end data type applied to the resultant :class:`DataFrame`\n",
      "        (still experimental). Behaviour is as follows:\n",
      "\n",
      "        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "          (default).\n",
      "        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "          DataFrame.\n",
      "\n",
      "        .. versionadded:: 2.0\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextFileReader\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_table : Read general delimited file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n",
      "\n",
      "Archivo cargado correctamente. Información general:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           5 non-null      int64 \n",
      " 1   Nombre       5 non-null      object\n",
      " 2   2016         5 non-null      object\n",
      " 3   2017         5 non-null      object\n",
      " 4   Crecimiento  5 non-null      object\n",
      " 5   Unidades     5 non-null      object\n",
      " 6   fecha        5 non-null      object\n",
      " 7   Activo       5 non-null      int64 \n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 452.0+ bytes\n",
      "None\n",
      "\n",
      "Vista previa de los datos:\n",
      "       ID        Nombre         2016          2017 Crecimiento Unidades  \\\n",
      "0   10002     Verde Mar  $125,000.00    $162500.00      30.00%      500   \n",
      "1  552278  Manantial sa  $920,000.00  $101,2000.00      10.00%      700   \n",
      "2   23477          ACME   $50,000.00      62500.00      25.00%      125   \n",
      "3   24900     Andes sur  $350,000.00     490000.00       4.00%       75   \n",
      "4  651029     San Pablo   $15,000.00     $12750.00     -15.00%       No   \n",
      "\n",
      "        fecha  Activo  \n",
      "0   1-10-2015       1  \n",
      "1   6-23-2014       0  \n",
      "2   3-12-2016       1  \n",
      "3  10-28-2015       1  \n",
      "4   2-15-2014       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "archivo = r'C:\\Users\\jorge\\ejemplo_data.csv'\n",
    "\n",
    "# Mostrar las opciones disponibles de pd.read_csv\n",
    "print(\"Opciones disponibles para pd.read_csv:\")\n",
    "help(pd.read_csv)\n",
    "\n",
    "# Leer el archivo CSV con algunas opciones útiles\n",
    "try:\n",
    "    # Leer el archivo y mostrar una vista previa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='utf-8',  # Codificación del archivo\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nArchivo cargado correctamente. Información general:\")\n",
    "    print(data.info())\n",
    "    print(\"\\nVista previa de los datos:\")\n",
    "    print(data.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificar tipos de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivo cargado correctamente. Información general:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           5 non-null      int64 \n",
      " 1   Nombre       5 non-null      object\n",
      " 2   2016         5 non-null      object\n",
      " 3   2017         5 non-null      object\n",
      " 4   Crecimiento  5 non-null      object\n",
      " 5   Unidades     5 non-null      object\n",
      " 6   fecha        5 non-null      object\n",
      " 7   Activo       5 non-null      int64 \n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 452.0+ bytes\n",
      "None\n",
      "\n",
      "Tipos de variables en la base de datos: AQUIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "ID              int64\n",
      "Nombre         object\n",
      "2016           object\n",
      "2017           object\n",
      "Crecimiento    object\n",
      "Unidades       object\n",
      "fecha          object\n",
      "Activo          int64\n",
      "dtype: object\n",
      "\n",
      "Vista previa de los datos:\n",
      "       ID        Nombre         2016          2017 Crecimiento Unidades  \\\n",
      "0   10002     Verde Mar  $125,000.00    $162500.00      30.00%      500   \n",
      "1  552278  Manantial sa  $920,000.00  $101,2000.00      10.00%      700   \n",
      "2   23477          ACME   $50,000.00      62500.00      25.00%      125   \n",
      "3   24900     Andes sur  $350,000.00     490000.00       4.00%       75   \n",
      "4  651029     San Pablo   $15,000.00     $12750.00     -15.00%       No   \n",
      "\n",
      "        fecha  Activo  \n",
      "0   1-10-2015       1  \n",
      "1   6-23-2014       0  \n",
      "2   3-12-2016       1  \n",
      "3  10-28-2015       1  \n",
      "4   2-15-2014       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "archivo = r'C:\\Users\\jorge\\ejemplo_data.csv'\n",
    "\n",
    "try:\n",
    "    # Leer el archivo y mostrar una vista previa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='utf-8',  # Codificación del archivo\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "    \n",
    "    # Mostrar la información general del DataFrame\n",
    "    print(\"\\nArchivo cargado correctamente. Información general:\")\n",
    "    print(data.info())\n",
    "\n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos: AQUIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Mostrar una vista previa de los datos\n",
    "    print(\"\\nVista previa de los datos:\")\n",
    "    print(data.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformar el atributo \"ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de variables en la base de datos antes de la transformación:\n",
      "ID              int64\n",
      "Nombre         object\n",
      "2016           object\n",
      "2017           object\n",
      "Crecimiento    object\n",
      "Unidades       object\n",
      "fecha          object\n",
      "Activo          int64\n",
      "dtype: object\n",
      "\n",
      "Tipos de variables después de la transformación:\n",
      "ID              int64\n",
      "Nombre         object\n",
      "2016           object\n",
      "2017           object\n",
      "Crecimiento    object\n",
      "Unidades       object\n",
      "fecha          object\n",
      "Activo           bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo CSV\n",
    "archivo = r'C:\\Users\\jorge\\ejemplo_data.csv'\n",
    "\n",
    "\n",
    "# Leer el archivo CSV con algunas opciones útiles\n",
    "try:\n",
    "    # Leer el archivo y mostrar una vista previa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='utf-8',  # Codificación del archivo\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "    \n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos antes de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Transformar \"ID\" a entero y \"Activo\" a booleano\n",
    "    data['ID'] = data['ID'].astype(int)\n",
    "    data['Activo'] = data['Activo'].astype(bool)\n",
    "\n",
    "    # Verificar los cambios\n",
    "    print(\"\\nTipos de variables después de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierta el atributo \"unidades\" a entero y \"2016\" a fotante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de variables en la base de datos antes de la transformación:\n",
      "ID              int64\n",
      "Nombre         object\n",
      "2016           object\n",
      "2017           object\n",
      "Crecimiento    object\n",
      "Unidades       object\n",
      "fecha          object\n",
      "Activo          int64\n",
      "dtype: object\n",
      "\n",
      "Tipos de variables después de la transformación:\n",
      "ID               int64\n",
      "Nombre          object\n",
      "2016           float64\n",
      "2017            object\n",
      "Crecimiento     object\n",
      "Unidades         int64\n",
      "fecha           object\n",
      "Activo            bool\n",
      "dtype: object\n",
      "\n",
      "Vista previa de los datos transformados:\n",
      "       ID        Nombre  2016          2017 Crecimiento  Unidades       fecha  \\\n",
      "0   10002     Verde Mar   0.0    $162500.00      30.00%       500   1-10-2015   \n",
      "1  552278  Manantial sa   0.0  $101,2000.00      10.00%       700   6-23-2014   \n",
      "2   23477          ACME   0.0      62500.00      25.00%       125   3-12-2016   \n",
      "3   24900     Andes sur   0.0     490000.00       4.00%        75  10-28-2015   \n",
      "4  651029     San Pablo   0.0     $12750.00     -15.00%         0   2-15-2014   \n",
      "\n",
      "   Activo  \n",
      "0    True  \n",
      "1   False  \n",
      "2    True  \n",
      "3    True  \n",
      "4   False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo CSV\n",
    "archivo = r'C:\\Users\\jorge\\ejemplo_data.csv'\n",
    "\n",
    "# Leer el archivo CSV con algunas opciones útiles\n",
    "try:\n",
    "    # Leer el archivo y mostrar una vista previa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='utf-8',  # Codificación del archivo\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "    \n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos antes de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Manejar valores no convertibles en \"Unidades\" y \"2016\"\n",
    "    # Reemplazar valores no numéricos con NaN\n",
    "    data['Unidades'] = pd.to_numeric(data['Unidades'], errors='coerce')\n",
    "    data['2016'] = pd.to_numeric(data['2016'], errors='coerce')\n",
    "\n",
    "    # Reemplazar NaN por un valor predeterminado (opcional, aquí 0)\n",
    "    data['Unidades'] = data['Unidades'].fillna(0).astype(int)\n",
    "    data['2016'] = data['2016'].fillna(0.0).astype(float)\n",
    "\n",
    "    # Transformar otras columnas\n",
    "    data['ID'] = data['ID'].astype(int)\n",
    "    data['Activo'] = data['Activo'].astype(bool)\n",
    "\n",
    "    # Verificar los cambios\n",
    "    print(\"\\nTipos de variables después de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Vista previa de los datos transformados\n",
    "    print(\"\\nVista previa de los datos transformados:\")\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado.\")\n",
    "except ValueError as ve:\n",
    "    print(f\"Error de conversión de datos: {ve}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Lectura y analisis exploratorio de datos 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opciones disponibles para pd.read_csv:\n",
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "\n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "\n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "\n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
      "        C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator from only the first valid\n",
      "        row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
      "        In addition, separators longer than 1 character and different from\n",
      "        ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
      "        the use of the Python parsing engine. Note that regex delimiters are prone\n",
      "        to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, optional\n",
      "        Alias for ``sep``.\n",
      "    header : int, Sequence of int, 'infer' or None, default 'infer'\n",
      "        Row number(s) containing column labels and marking the start of the\n",
      "        data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly to ``names`` then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
      "        e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : Sequence of Hashable, optional\n",
      "        Sequence of column labels to apply. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : Hashable, Sequence of Hashable or False, optional\n",
      "      Column(s) to use as row label(s), denoted either by column labels or column\n",
      "      indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
      "      will be formed for the row labels.\n",
      "\n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g., when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : Sequence of Hashable or Callable, optional\n",
      "        Subset of columns to select, denoted either by column labels or column indices.\n",
      "        If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in ``names`` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
      "        preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
      "        for columns in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to ``True``. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    dtype : dtype or dict of {Hashable : dtype}, optional\n",
      "        Data type(s) to apply to either the whole dataset or individual columns.\n",
      "        E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n",
      "        Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
      "        to preserve and not interpret ``dtype``.\n",
      "        If ``converters`` are specified, they will be applied INSTEAD\n",
      "        of ``dtype`` conversion.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "            Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
      "            the default determines the ``dtype`` of the columns which are not explicitly\n",
      "            listed.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict of {Hashable : Callable}, optional\n",
      "        Functions for converting values in specified columns. Keys can either\n",
      "        be column labels or column indices.\n",
      "    true_values : list, optional\n",
      "        Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
      "    false_values : list, optional\n",
      "        Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : int, list of int or Callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
      "        at the start of the file.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
      "        Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
      "        per-column ``NA`` values.  By default the following values are interpreted as\n",
      "        ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n",
      "        \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n",
      "        \"n/a\", \"nan\", \"null \".\n",
      "\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default ``NaN`` values when parsing the data.\n",
      "        Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
      "\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
      "          is appended to the default ``NaN`` values used for parsing.\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
      "          the default ``NaN`` values are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
      "          the ``NaN`` values specified ``na_values`` are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
      "          strings will be parsed as ``NaN``.\n",
      "\n",
      "        Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
      "        ``na_values`` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of ``na_values``). In\n",
      "        data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
      "        performance of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of ``NA`` values placed in non-numeric columns.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "    skip_blank_lines : bool, default True\n",
      "        If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
      "    parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
      "        The behavior is as follows:\n",
      "\n",
      "        * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
      "          ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
      "        * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
      "          as a single date column. Values are joined with a space before parsing.\n",
      "        * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n",
      "          result 'foo'. Values are joined with a space before parsing.\n",
      "\n",
      "        If a column or index cannot be represented as an array of ``datetime``,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an ``object`` data type. For\n",
      "        non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
      "        :func:`~pandas.read_csv`.\n",
      "\n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
      "        format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "            A strict version of this argument is now the default, passing it has no effect.\n",
      "\n",
      "    keep_date_col : bool, default False\n",
      "        If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : Callable, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. pandas will try to call ``date_parser`` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by ``parse_dates`` into a single array\n",
      "        and pass that; and 3) call ``date_parser`` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by ``parse_dates``) as\n",
      "        arguments.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "           Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "           :func:`~pandas.to_datetime` as-needed.\n",
      "    date_format : str or dict of column -> format, optional\n",
      "        Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
      "        The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "        `strftime documentation\n",
      "        <https://docs.python.org/3/library/datetime.html\n",
      "        #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "        note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "        You can also pass:\n",
      "\n",
      "        - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "            time string (not necessarily in exactly the same format);\n",
      "        - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "            and you should probably use it along with `dayfirst`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "\n",
      "    iterator : bool, default False\n",
      "        Return ``TextFileReader`` object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Number of lines to read from the file per chunk. Passing a value will cause the\n",
      "        function to return a ``TextFileReader`` object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
      "        Set to ``None`` for no decompression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "        other key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "\n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "    thousands : str (length 1), optional\n",
      "        Character acting as the thousands separator in numerical values.\n",
      "    decimal : str (length 1), default '.'\n",
      "        Character to recognize as decimal point (e.g., use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character used to denote a line break. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        Character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the ``delimiter`` and it will be ignored.\n",
      "    quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
      "        ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
      "        characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
      "        or ``lineterminator``.\n",
      "    doublequote : bool, default True\n",
      "       When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        Character used to escape other characters.\n",
      "    comment : str (length 1), optional\n",
      "        Character indicating that the remainder of line should not be parsed.\n",
      "        If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter ``header`` but not by\n",
      "        ``skiprows``. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
      "        treated as the header.\n",
      "    encoding : str, optional, default 'utf-8'\n",
      "        Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "\n",
      "    encoding_errors : str, optional, default 'strict'\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
      "        ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
      "        override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
      "        documentation for more details.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "\n",
      "        - ``'error'``, raise an Exception when a bad line is encountered.\n",
      "        - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
      "        - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new ``list`` of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine='python'``\n",
      "\n",
      "        .. versionchanged:: 2.2.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              as described in `pyarrow documentation\n",
      "              <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
      "              #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
      "\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
      "        used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to ``True``, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "            Use ``sep=\"\\s+\"`` instead.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
      "        Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
      "        regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
      "        chunks. (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : {'high', 'legacy', 'round_trip'}, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
      "        ``'legacy'`` for the original lower precision pandas converter, and\n",
      "        ``'round_trip'`` for the round-trip converter.\n",
      "\n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "    dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
      "        Back-end data type applied to the resultant :class:`DataFrame`\n",
      "        (still experimental). Behaviour is as follows:\n",
      "\n",
      "        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "          (default).\n",
      "        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "          DataFrame.\n",
      "\n",
      "        .. versionadded:: 2.0\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextFileReader\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_table : Read general delimited file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n",
      "Ocurrió un error al cargar el archivo: 'utf-8' codec can't decode byte 0xa3 in position 28: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "archivo = r'C:\\Users\\jorge\\ecommerce_data.csv'\n",
    "\n",
    "# Mostrar las opciones disponibles de pd.read_csv\n",
    "print(\"Opciones disponibles para pd.read_csv:\")\n",
    "help(pd.read_csv)\n",
    "\n",
    "# Leer el archivo CSV con algunas opciones útiles\n",
    "try:\n",
    "    # Leer el archivo y mostrar una vista previa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='utf-8',  # Codificación del archivo\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nArchivo cargado correctamente. Información general:\")\n",
    "    print(data.info())\n",
    "    print(\"\\nVista previa de los datos:\")\n",
    "    print(data.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 IDENTIFICAR TIPO DE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivo cargado correctamente. Información general:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n",
      "None\n",
      "\n",
      "Tipos de variables en la base de datos:\n",
      "InvoiceNo       object\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "dtype: object\n",
      "\n",
      "Vista previa de los datos:\n",
      "  InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1    536365     71053                  WHITE METAL LANTERN         6   \n",
      "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
      "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo\n",
    "archivo = r'C:\\Users\\jorge\\ecommerce_data.csv'\n",
    "\n",
    "# Leer el archivo CSV con manejo de codificación\n",
    "try:\n",
    "    # Leer el archivo con una codificación alternativa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='latin-1',  # Codificación alternativa\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "    \n",
    "    # Mostrar la información general del DataFrame\n",
    "    print(\"\\nArchivo cargado correctamente. Información general:\")\n",
    "    print(data.info())\n",
    "\n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Mostrar una vista previa de los datos\n",
    "    print(\"\\nVista previa de los datos:\")\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Archivo no encontrado en la ruta '{archivo}'.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error de codificación: {e}. Prueba con otra codificación, como 'ISO-8859-1'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la funcion astype transforme el atributo \"InvoiceNo\" a entero y el atributo \"Description\"\n",
    "a string. Vuelva a consultar el estado de las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de variables en la base de datos ANTES de la transformación:\n",
      "InvoiceNo       object\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "dtype: object\n",
      "\n",
      "Tipos de variables DESPUÉS de la transformación:\n",
      "InvoiceNo        int64\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "dtype: object\n",
      "\n",
      "Vista previa de los datos transformados:\n",
      "   InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0     536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1     536365     71053                  WHITE METAL LANTERN         6   \n",
      "2     536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3     536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4     536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
      "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo\n",
    "archivo = r'C:\\Users\\jorge\\ecommerce_data.csv'\n",
    "\n",
    "# Leer el archivo CSV con manejo de codificación\n",
    "try:\n",
    "    # Leer el archivo con una codificación alternativa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='latin-1',  # Codificación alternativa\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "\n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos ANTES de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Filtrar filas donde InvoiceNo contiene solo valores numéricos\n",
    "    data = data[data['InvoiceNo'].str.isnumeric()]\n",
    "\n",
    "    # Transformar \"InvoiceNo\" a entero y \"Description\" a cadena\n",
    "    try:\n",
    "        data['InvoiceNo'] = data['InvoiceNo'].astype(int)\n",
    "        data['Description'] = data['Description'].astype(str)\n",
    "\n",
    "        # Verificar los cambios\n",
    "        print(\"\\nTipos de variables DESPUÉS de la transformación:\")\n",
    "        print(data.dtypes)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Una de las columnas no existe en los datos ({e}).\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error de conversión: {e}\")\n",
    "\n",
    "    # Mostrar una vista previa de los datos transformados\n",
    "    print(\"\\nVista previa de los datos transformados:\")\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Archivo no encontrado en la ruta '{archivo}'.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error de codificación: {e}. Prueba con otra codificación, como 'ISO-8859-1'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierta el atributo \"Quantity\" a entero y \"UnitPrice\" a \n",
    "otante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de variables en la base de datos ANTES de la transformación:\n",
      "InvoiceNo       object\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "dtype: object\n",
      "\n",
      "Tipos de variables DESPUÉS de la transformación:\n",
      "InvoiceNo        int64\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "dtype: object\n",
      "\n",
      "Vista previa de los datos transformados:\n",
      "   InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0     536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1     536365     71053                  WHITE METAL LANTERN         6   \n",
      "2     536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3     536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4     536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
      "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo\n",
    "archivo = r'C:\\Users\\jorge\\ecommerce_data.csv'\n",
    "\n",
    "# Leer el archivo CSV con manejo de codificación\n",
    "try:\n",
    "    # Leer el archivo con una codificación alternativa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='latin-1',  # Codificación alternativa\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "\n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos ANTES de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Filtrar filas donde InvoiceNo contiene solo valores numéricos\n",
    "    data = data[data['InvoiceNo'].str.isnumeric()]\n",
    "\n",
    "    # Transformar \"InvoiceNo\" a entero, \"Description\" a cadena, \"Quantity\" a entero y \"UnitPrice\" a flotante\n",
    "    try:\n",
    "        data['InvoiceNo'] = data['InvoiceNo'].astype(int)\n",
    "        data['Description'] = data['Description'].astype(str)\n",
    "        data['Quantity'] = data['Quantity'].astype(int)  # Convertir Quantity a entero\n",
    "        data['UnitPrice'] = data['UnitPrice'].astype(float)  # Convertir UnitPrice a flotante\n",
    "\n",
    "        # Verificar los cambios\n",
    "        print(\"\\nTipos de variables DESPUÉS de la transformación:\")\n",
    "        print(data.dtypes)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Una de las columnas no existe en los datos ({e}).\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error de conversión: {e}\")\n",
    "\n",
    "    # Mostrar una vista previa de los datos transformados\n",
    "    print(\"\\nVista previa de los datos transformados:\")\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Archivo no encontrado en la ruta '{archivo}'.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error de codificación: {e}. Prueba con otra codificación, como 'ISO-8859-1'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna \"InvoiceDate\" contiene un string que representa \"fecha-hora\", separe la columna en\n",
    "dos columnas que representen cada atributo por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de variables en la base de datos ANTES de la transformación:\n",
      "InvoiceNo       object\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "dtype: object\n",
      "\n",
      "Tipos de variables DESPUÉS de la transformación:\n",
      "InvoiceNo        int64\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "Date            object\n",
      "Time            object\n",
      "dtype: object\n",
      "\n",
      "Vista previa de los datos transformados:\n",
      "   InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0     536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1     536365     71053                  WHITE METAL LANTERN         6   \n",
      "2     536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3     536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4     536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country       Date  Time  \n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  12/1/2010  8:26  \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26  \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  12/1/2010  8:26  \n",
      "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26  \n",
      "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo\n",
    "archivo = r'C:\\Users\\jorge\\ecommerce_data.csv'\n",
    "\n",
    "# Leer el archivo CSV con manejo de codificación\n",
    "try:\n",
    "    # Leer el archivo con una codificación alternativa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='latin-1',  # Codificación alternativa\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "\n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos ANTES de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Filtrar filas donde InvoiceNo contiene solo valores numéricos\n",
    "    data = data[data['InvoiceNo'].str.isnumeric()]\n",
    "\n",
    "    # Transformar columnas\n",
    "    try:\n",
    "        data['InvoiceNo'] = data['InvoiceNo'].astype(int)\n",
    "        data['Description'] = data['Description'].astype(str)\n",
    "        data['Quantity'] = data['Quantity'].astype(int)  # Convertir Quantity a entero\n",
    "        data['UnitPrice'] = data['UnitPrice'].astype(float)  # Convertir UnitPrice a flotante\n",
    "\n",
    "        # Separar la columna \"InvoiceDate\" en \"Date\" y \"Time\"\n",
    "        data[['Date', 'Time']] = data['InvoiceDate'].str.split(' ', expand=True)\n",
    "\n",
    "        # Verificar los cambios\n",
    "        print(\"\\nTipos de variables DESPUÉS de la transformación:\")\n",
    "        print(data.dtypes)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Una de las columnas no existe en los datos ({e}).\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error de conversión: {e}\")\n",
    "\n",
    "    # Mostrar una vista previa de los datos transformados\n",
    "    print(\"\\nVista previa de los datos transformados:\")\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Archivo no encontrado en la ruta '{archivo}'.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error de codificación: {e}. Prueba con otra codificación, como 'ISO-8859-1'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anada una nueva columna que represente el monto total para cada boleta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de variables en la base de datos ANTES de la transformación:\n",
      "InvoiceNo       object\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "dtype: object\n",
      "\n",
      "Tipos de variables DESPUÉS de la transformación:\n",
      "InvoiceNo        int64\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "Date            object\n",
      "Time            object\n",
      "TotalAmount    float64\n",
      "dtype: object\n",
      "\n",
      "Vista previa de los datos transformados:\n",
      "   InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0     536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1     536365     71053                  WHITE METAL LANTERN         6   \n",
      "2     536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3     536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4     536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country       Date  Time  \\\n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "\n",
      "   TotalAmount  \n",
      "0        15.30  \n",
      "1        20.34  \n",
      "2        22.00  \n",
      "3        20.34  \n",
      "4        20.34  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo\n",
    "archivo = r'C:\\Users\\jorge\\ecommerce_data.csv'\n",
    "\n",
    "# Leer el archivo CSV con manejo de codificación\n",
    "try:\n",
    "    # Leer el archivo con una codificación alternativa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='latin-1',  # Codificación alternativa\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "\n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos ANTES de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Filtrar filas donde InvoiceNo contiene solo valores numéricos\n",
    "    data = data[data['InvoiceNo'].str.isnumeric()]\n",
    "\n",
    "    # Transformar columnas\n",
    "    try:\n",
    "        data['InvoiceNo'] = data['InvoiceNo'].astype(int)\n",
    "        data['Description'] = data['Description'].astype(str)\n",
    "        data['Quantity'] = data['Quantity'].astype(int)  # Convertir Quantity a entero\n",
    "        data['UnitPrice'] = data['UnitPrice'].astype(float)  # Convertir UnitPrice a flotante\n",
    "\n",
    "        # Separar la columna \"InvoiceDate\" en \"Date\" y \"Time\"\n",
    "        data[['Date', 'Time']] = data['InvoiceDate'].str.split(' ', expand=True)\n",
    "\n",
    "        # Añadir una nueva columna para el monto total\n",
    "        data['TotalAmount'] = data['Quantity'] * data['UnitPrice']\n",
    "\n",
    "        # Verificar los cambios\n",
    "        print(\"\\nTipos de variables DESPUÉS de la transformación:\")\n",
    "        print(data.dtypes)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Una de las columnas no existe en los datos ({e}).\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error de conversión: {e}\")\n",
    "\n",
    "    # Mostrar una vista previa de los datos transformados\n",
    "    print(\"\\nVista previa de los datos transformados:\")\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Archivo no encontrado en la ruta '{archivo}'.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error de codificación: {e}. Prueba con otra codificación, como 'ISO-8859-1'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporte la base de datos procesada en formato \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de variables en la base de datos ANTES de la transformación:\n",
      "InvoiceNo       object\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "dtype: object\n",
      "\n",
      "Base de datos procesada exportada correctamente a: C:\\Users\\jorge\\ecommerce_data_processed.csv\n",
      "\n",
      "Tipos de variables DESPUÉS de la transformación:\n",
      "InvoiceNo        int64\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID     float64\n",
      "Country         object\n",
      "Date            object\n",
      "Time            object\n",
      "TotalAmount    float64\n",
      "dtype: object\n",
      "\n",
      "Vista previa de los datos transformados:\n",
      "   InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0     536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1     536365     71053                  WHITE METAL LANTERN         6   \n",
      "2     536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3     536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4     536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country       Date  Time  \\\n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  12/1/2010  8:26   \n",
      "\n",
      "   TotalAmount  \n",
      "0        15.30  \n",
      "1        20.34  \n",
      "2        22.00  \n",
      "3        20.34  \n",
      "4        20.34  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo\n",
    "archivo = r'C:\\Users\\jorge\\ecommerce_data.csv'\n",
    "\n",
    "# Leer el archivo CSV con manejo de codificación\n",
    "try:\n",
    "    # Leer el archivo con una codificación alternativa\n",
    "    data = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=',',           # Separador de columnas (por defecto es ',')\n",
    "        header=0,          # Fila a usar como encabezado\n",
    "        encoding='latin-1',  # Codificación alternativa\n",
    "        na_values=['NA'],  # Valores que se interpretarán como NaN\n",
    "        skiprows=0,        # Número de filas a omitir al principio\n",
    "        nrows=None         # Número de filas a leer (None para todas)\n",
    "    )\n",
    "\n",
    "    # Identificar los tipos de variables\n",
    "    print(\"\\nTipos de variables en la base de datos ANTES de la transformación:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Filtrar filas donde InvoiceNo contiene solo valores numéricos\n",
    "    data = data[data['InvoiceNo'].str.isnumeric()]\n",
    "\n",
    "    # Transformar columnas\n",
    "    try:\n",
    "        data['InvoiceNo'] = data['InvoiceNo'].astype(int)\n",
    "        data['Description'] = data['Description'].astype(str)\n",
    "        data['Quantity'] = data['Quantity'].astype(int)  # Convertir Quantity a entero\n",
    "        data['UnitPrice'] = data['UnitPrice'].astype(float)  # Convertir UnitPrice a flotante\n",
    "\n",
    "        # Separar la columna \"InvoiceDate\" en \"Date\" y \"Time\"\n",
    "        data[['Date', 'Time']] = data['InvoiceDate'].str.split(' ', expand=True)\n",
    "\n",
    "        # Añadir una nueva columna para el monto total\n",
    "        data['TotalAmount'] = data['Quantity'] * data['UnitPrice']\n",
    "\n",
    "        # Exportar la base de datos procesada\n",
    "        output_file = r'C:\\Users\\jorge\\ecommerce_data_processed.csv'\n",
    "        data.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"\\nBase de datos procesada exportada correctamente a: {output_file}\")\n",
    "\n",
    "        # Verificar los cambios\n",
    "        print(\"\\nTipos de variables DESPUÉS de la transformación:\")\n",
    "        print(data.dtypes)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Una de las columnas no existe en los datos ({e}).\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error de conversión: {e}\")\n",
    "\n",
    "    # Mostrar una vista previa de los datos transformados\n",
    "    print(\"\\nVista previa de los datos transformados:\")\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Archivo no encontrado en la ruta '{archivo}'.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error de codificación: {e}. Prueba con otra codificación, como 'ISO-8859-1'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Estadsticas descriptivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: [1, 2, 3, 4, 5] ...\n",
      "Atributo1: [34.00692036534253, 42.4471859120852, 40.55428879617493, 23.23556840345045, 24.879429344035017] ...\n",
      "Atributo2: [0.15111931885076568, 0.4307740577612712, 0.5244068816490317, 0.5618494576677711, 0.6727353424903955] ...\n",
      "Atributo3: [297.3764216610786, 286.5542617597016, 464.08847942329453, 133.7162550296518, 82.26613097598846] ...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Crear un diccionario con 50 registros\n",
    "data = {\n",
    "    \"ID\": [i + 1 for i in range(50)],  # Atributo discreto: ID del 1 al 50\n",
    "    \"Atributo1\": [random.uniform(10, 100) for _ in range(50)],  # Continuo: Valores entre 10 y 100\n",
    "    \"Atributo2\": [random.uniform(0, 1) for _ in range(50)],  # Continuo: Valores entre 0 y 1\n",
    "    \"Atributo3\": [random.uniform(50, 500) for _ in range(50)]  # Continuo: Valores entre 50 y 500\n",
    "}\n",
    "\n",
    "# Mostrar una vista previa del diccionario\n",
    "for key, values in data.items():\n",
    "    print(f\"{key}: {values[:5]} ...\")  # Mostrar las primeras 5 entradas por atributo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforme dicho diccionario a un dataFrame de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vista previa del DataFrame:\n",
      "   ID  Atributo1  Atributo2   Atributo3\n",
      "0   1  43.565859   0.679225  193.511382\n",
      "1   2  19.110326   0.057356  183.704517\n",
      "2   3  97.685518   0.349759  451.678435\n",
      "3   4  40.162468   0.855384  384.189663\n",
      "4   5  93.050311   0.411389  419.323398\n",
      "\n",
      "Información del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ID         50 non-null     int64  \n",
      " 1   Atributo1  50 non-null     float64\n",
      " 2   Atributo2  50 non-null     float64\n",
      " 3   Atributo3  50 non-null     float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 1.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Crear un diccionario con 50 registros\n",
    "data = {\n",
    "    \"ID\": [i + 1 for i in range(50)],  # Atributo discreto: ID del 1 al 50\n",
    "    \"Atributo1\": [random.uniform(10, 100) for _ in range(50)],  # Continuo: Valores entre 10 y 100\n",
    "    \"Atributo2\": [random.uniform(0, 1) for _ in range(50)],  # Continuo: Valores entre 0 y 1\n",
    "    \"Atributo3\": [random.uniform(50, 500) for _ in range(50)]  # Continuo: Valores entre 50 y 500\n",
    "}\n",
    "\n",
    "# Transformar el diccionario en un DataFrame de pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar una vista previa del DataFrame\n",
    "print(\"\\nVista previa del DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Mostrar información general del DataFrame\n",
    "print(\"\\nInformación del DataFrame:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenga estadisticas descriptivas de tendencia central."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estadísticas descriptivas de tendencia central:\n",
      "        Atributo1  Atributo2   Atributo3\n",
      "mean    58.282299   0.522355  274.172697\n",
      "median  60.291312   0.469627  286.234207\n",
      "std     25.649891   0.296906  125.274118\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Crear un diccionario con 50 registros\n",
    "data = {\n",
    "    \"ID\": [i + 1 for i in range(50)],  # Atributo discreto: ID del 1 al 50\n",
    "    \"Atributo1\": [random.uniform(10, 100) for _ in range(50)],  # Continuo: Valores entre 10 y 100\n",
    "    \"Atributo2\": [random.uniform(0, 1) for _ in range(50)],  # Continuo: Valores entre 0 y 1\n",
    "    \"Atributo3\": [random.uniform(50, 500) for _ in range(50)]  # Continuo: Valores entre 50 y 500\n",
    "}\n",
    "\n",
    "# Transformar el diccionario en un DataFrame de pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Estadísticas descriptivas de tendencia central\n",
    "print(\"\\nEstadísticas descriptivas de tendencia central:\")\n",
    "central_tendency_stats = df[[\"Atributo1\", \"Atributo2\", \"Atributo3\"]].agg(['mean', 'median', 'std'])\n",
    "print(central_tendency_stats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenga estadisticas descriptivas de dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vista previa del DataFrame:\n",
      "   ID  Atributo1  Atributo2   Atributo3\n",
      "0   1  15.266829   0.268636  124.771136\n",
      "1   2  42.467080   0.788838  407.104831\n",
      "2   3  12.213295   0.987308  477.182289\n",
      "3   4  53.337853   0.535713   81.561040\n",
      "4   5  10.447835   0.621193  151.080973\n",
      "\n",
      "Estadísticas descriptivas de tendencia central:\n",
      "        Atributo1  Atributo2   Atributo3\n",
      "mean    53.673012   0.510956  261.051454\n",
      "median  53.057848   0.538121  236.723496\n",
      "std     26.356471   0.296590  136.754586\n",
      "\n",
      "Estadísticas descriptivas de dispersión:\n",
      "        Atributo1  Atributo2     Atributo3\n",
      "var    694.663574   0.087966  18701.816871\n",
      "min     10.447835   0.022470     52.259542\n",
      "max     99.370370   0.987330    494.485862\n",
      "range   88.922536   0.964860    442.226320\n",
      "\n",
      "Información del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ID         50 non-null     int64  \n",
      " 1   Atributo1  50 non-null     float64\n",
      " 2   Atributo2  50 non-null     float64\n",
      " 3   Atributo3  50 non-null     float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 1.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Crear un diccionario con 50 registros\n",
    "data = {\n",
    "    \"ID\": [i + 1 for i in range(50)],  # Atributo discreto: ID del 1 al 50\n",
    "    \"Atributo1\": [random.uniform(10, 100) for _ in range(50)],  # Continuo: Valores entre 10 y 100\n",
    "    \"Atributo2\": [random.uniform(0, 1) for _ in range(50)],  # Continuo: Valores entre 0 y 1\n",
    "    \"Atributo3\": [random.uniform(50, 500) for _ in range(50)]  # Continuo: Valores entre 50 y 500\n",
    "}\n",
    "\n",
    "# Transformar el diccionario en un DataFrame de pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar una vista previa del DataFrame\n",
    "print(\"\\nVista previa del DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Estadísticas descriptivas de tendencia central\n",
    "print(\"\\nEstadísticas descriptivas de tendencia central:\")\n",
    "central_tendency_stats = df[[\"Atributo1\", \"Atributo2\", \"Atributo3\"]].agg(['mean', 'median', 'std'])\n",
    "print(central_tendency_stats)\n",
    "\n",
    "# Estadísticas descriptivas de dispersión\n",
    "print(\"\\nEstadísticas descriptivas de dispersión:\")\n",
    "dispersion_stats = df[[\"Atributo1\", \"Atributo2\", \"Atributo3\"]].agg(['var', 'min', 'max'])\n",
    "dispersion_stats.loc['range'] = dispersion_stats.loc['max'] - dispersion_stats.loc['min']  # Calcular rango manualmente\n",
    "print(dispersion_stats)\n",
    "\n",
    "# Mostrar información general del DataFrame\n",
    "print(\"\\nInformación del DataFrame:\")\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacion e imputacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnóstico de números perdidos en 'ratings data.csv':\n",
      "Unnamed: 0     0\n",
      "User-ID        0\n",
      "ISBN           0\n",
      "Book-Rating    0\n",
      "MeanRating     0\n",
      "dtype: int64\n",
      "\n",
      "Filas con valores nulos en 'ratings data.csv':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rutas locales de los archivos\n",
    "ratings_file = r'C:\\Users\\jorge\\ratings_data.csv'\n",
    "books_file = r'C:\\Users\\jorge\\books_data.csv'\n",
    "\n",
    "try:\n",
    "    # Leer los archivos CSV con manejo de errores\n",
    "    ratings_data = pd.read_csv(ratings_file, encoding='latin-1', sep=',', on_bad_lines='skip')\n",
    "    books_data = pd.read_csv(books_file, encoding='latin-1', sep=';')\n",
    "\n",
    "    # Diagnóstico de números perdidos en \"ratings data.csv\"\n",
    "    print(\"\\nDiagnóstico de números perdidos en 'ratings data.csv':\")\n",
    "    missing_diagnostics = ratings_data.isnull().sum()\n",
    "    print(missing_diagnostics)\n",
    "\n",
    "    # Identificar filas con valores nulos\n",
    "    print(\"\\nFilas con valores nulos en 'ratings data.csv':\")\n",
    "    print(ratings_data[ratings_data.isnull().any(axis=1)])\n",
    "\n",
    "    # Imputación de valores según la media\n",
    "    print(\"\\nImputando valores nulos según la media:\")\n",
    "    ratings_data_mean = ratings_data.copy()\n",
    "    ratings_data_mean.fillna(ratings_data_mean.mean(), inplace=True)\n",
    "    print(ratings_data_mean.head())\n",
    "\n",
    "    # Imputación de valores según otro criterio: valor constante\n",
    "    print(\"\\nImputando valores nulos con un valor constante (0):\")\n",
    "    ratings_data_constant = ratings_data.copy()\n",
    "    ratings_data_constant.fillna(0, inplace=True)\n",
    "    print(ratings_data_constant.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al procesar los archivos: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
